[[multi-region-two-datacenter-architecture]]
=== Multi-Region - Two Datacenters

This article defines a scalable and highly available architecture for Elasticsearch using two datacenters in separate geographical regions. The architecture includes all the necessary components of the Elastic Stack and is not intended for sizing workloads, but rather as a basis to ensure the architecture you deploy is foundationally ready to handle any desired workload with resiliency. This architecture does include very high level representations of data flow, but the implementation of which will be included in subsequent documentation.

[discrete]
[[multi-region-use-case]]
==== Use Case

This architecture is intended for organizations that need to: 

* Monitor the performance and health of their applications in real-time
* Provide insights and alerts to ensure optimal performance and quick issue resolution for applications

[discrete]
[[multi-region-architecture]]
==== Architecture

image::images/multi-region-two-datacenter.png["A multi-region time-series architecture across two datacenters"]

[discrete]
[[multi-region-considerations]]
==== Important Considerations

The following list are important conderations for this architecture:

* **Shard Management** 
** The most important foundational step to maintaining performance as you scale is proper shard sizing, location, count, and shard distribution. For a complete understanding of what shards are and how they should be used please review, https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html[Shard sizing].
*** **Sizing:** Maintain shard sizes within recommended ranges and aim for an optimal number of shards.
*** **Distribution:** In a distributed system, any distributed process is only as fast as the slowest node. As a result, it is optimal to maintain indexes with a primary shard count that is a multiple of the node count in a given tier. This creates even distribution of processing and prevents hotspots.
**** Shard distribution should be enforced using the https://www.elastic.co/guide/en/elasticsearch/reference/current/size-your-shards.html#avoid-node-hotspots['total shards per node'] index level setting 

* **Index lifecyle:** Use index lifecycle management with index templates for consistent index level settings, please see, https://www.elastic.co/guide/en/elasticsearch/reference/current/set-up-lifecycle-policy.html[Configure a lifecycle policy] for more detail.
** *Hot:* Use this tier for ingestion and the fastest reads on the most current data. This architecture assumes no updates to the data once written.
** **Warm / Cold** - This tier is not considered for this pattern.
** **Frozen:** Data is persisted in a repository; however, it is accessed from the node's cache. It may not be as fast as the Hot tier; however, it can still be fast depending on the caching strategy. Frozen does not mean slow - it means immutable and saved in durable storage.

*** **Shard allocation awareness:** To prevent both a primary and a replica from being copied to the same zone, or in this case the same pod, you can use https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-cluster.html#shard-allocation-awareness[shard allocation awareness] and define a simple attribute in the elaticsearch.yaml file on a per-node basis to make Elasticsearch aware of the physical topology and route shards appropriately. In deployment models with multiple availability zones, AZ's would be used in place of pod location.

* **Limitations of this architecture**
** No region resilience

[discrete]
[[multi-region-resources]]
==== Resources and references

* <<shard-size-best-practices,Size your shards>>
* https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html[Elasticsearch Documentation]
* https://www.elastic.co/guide/en/kibana/current/index.html[Kibana Documentation]

