[role="xpack"]
[[inference-apis]]
== {infer-cap} APIs

IMPORTANT: The {infer} APIs enable you to use certain services, such as built-in
{ml} models (ELSER, E5), models uploaded through Eland, Cohere, OpenAI, Azure,
Google AI Studio or Hugging Face. For built-in models and models uploaded
through Eland, the {infer} APIs offer an alternative way to use and manage
trained models. However, if you do not plan to use the {infer} APIs to use these
models or if you want to use non-NLP models, use the
<<ml-df-trained-models-apis>>.

The {infer} APIs enable you to create {infer} endpoints and use {ml} models of
different providers - such as Amazon Bedrock, Anthropic, Azure AI Studio,
Cohere, Google AI, Mistral, OpenAI, or HuggingFace - as a service. Use
the following APIs to manage {infer} models and perform {infer}:

* <<delete-inference-api>>
* <<get-inference-api>>
* <<post-inference-api>>
* <<put-inference-api>>
* <<update-inference-api>>

[[inference-landscape]]
.A representation of the Elastic inference landscape
image::images/inference-landscape.jpg[A representation of the Elastic inference landscape,align="center"]

An {infer} endpoint enables you to use the corresponding {ml} model without
manual deployment and apply it to your data at ingestion time through
<<semantic-search-semantic-text, semantic text>>. 

Choose a model from your provider or use ELSER – a retrieval model trained by 
Elastic –, then create an {infer} endpoint by the <<put-inference-api>>.
Now use <<semantic-search-semantic-text, semantic text>> to perform
<<semantic-search, semantic search>> on your data.

[discrete]
[[infer-chunking-config]]
=== Configuring chunking

{infer-cap} endpoints have a limit on the amount of text they can process at once.
To allow for large amounts of text to be used, {infer} endpoints automatically split the text into smaller, more manageable passages if needed, called _chunks_.
Then chunks will be processed by the {infer} process.

Each chunk will include the text subpassage and the corresponding embedding generated from it.

By default, documents are split into 250-word sections with 1 sentence overlap so that each chunk shares a sentence with the previous chunk.
Overlapping ensures continuity and prevents vital contextual information in the input text from being lost by a hard break. 

[discrete]
==== Chunking strategies

Two strategies are available for chunking: `sentence` and `word`.

The `sentence` strategy considers sentences when chunking.

For `word` strategy, {es} uses the https://unicode-org.github.io/icu-docs/[ICU4J] library to detect word boundaries.
https://unicode-org.github.io/icu/userguide/boundaryanalysis/#word-boundary[Word boundaries] are identified by following a series of rules, not just the presence of a whitespace character.
For written languages that do use whitespace such as Chinese or Japanese dictionary lookups are used to detect word boundaries.






include::delete-inference.asciidoc[]
include::get-inference.asciidoc[]
include::post-inference.asciidoc[]
include::put-inference.asciidoc[]
include::update-inference.asciidoc[]
include::service-alibabacloud-ai-search.asciidoc[]
include::service-amazon-bedrock.asciidoc[]
include::service-anthropic.asciidoc[]
include::service-azure-ai-studio.asciidoc[]
include::service-azure-openai.asciidoc[]
include::service-cohere.asciidoc[]
include::service-elasticsearch.asciidoc[]
include::service-elser.asciidoc[]
include::service-google-ai-studio.asciidoc[]
include::service-google-vertex-ai.asciidoc[]
include::service-hugging-face.asciidoc[]
include::service-mistral.asciidoc[]
include::service-openai.asciidoc[]
include::service-watsonx-ai.asciidoc[]
